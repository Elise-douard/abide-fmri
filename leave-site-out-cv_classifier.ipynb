{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leave-site-out cross-validation\n",
    "\n",
    "This notebook will implement a classifier on the ABIDE dataset that predicts autism from resting state fMRI data using leave-site-out cross-validation. This means that each fold contains every site but one, which is then used to measure performance on.\n",
    "\n",
    "Let's first load the dataset so that we can extract the group information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikkel/miniconda3/lib/python3.7/site-packages/numpy/lib/npyio.py:2372: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  output = genfromtxt(fname, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "import pandas as pd\n",
    "#CHANGE PATH (data_dir) TO WERE YOU WANT TO STORE THE DATA\n",
    "abide = datasets.fetch_abide_pcp(data_dir=\"/mnt/c/brainhackschool/project/data\",\n",
    "                                 pipeline=\"cpac\",\n",
    "                                 quality_checked=True)\n",
    "\n",
    "# Transform phenotypic data into dataframe\n",
    "abide_pheno = pd.DataFrame(abide.phenotypic)\n",
    "\n",
    "# Extract group info\n",
    "groups = []\n",
    "for s in abide_pheno.SITE_ID:\n",
    "    groups.append(s.decode()) # for some reason the site names are of type 'bytes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define how to split our data set. Fortunately, there is a function in scikit-learn that splits data by groups, leaving one group in each iteration, which is exactly what we want here.\n",
    "\n",
    "We will use the `prepare_data` function from the `prepare_data.py` we wrote to get X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikkel/miniconda3/lib/python3.7/site-packages/numpy/lib/npyio.py:2372: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  output = genfromtxt(fname, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature file found.\n",
      "Running PCA...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import numpy as np\n",
    "import prepare_data\n",
    "import os\n",
    "\n",
    "# define data and output directories\n",
    "# if you already have downloaded the data, provide that directory\n",
    "# THIS NEEDS TO BE CHANGED IN ORDER TO WORK FOR YOU\n",
    "data_dir = os.path.join(os.sep, \"mnt\", \"c\", \"brainhackschool\", \"project\", \"data\")\n",
    "output_dir = data_dir\n",
    "\n",
    "X, y = prepare_data.prepare_data(data_dir, output_dir)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our classifier. The classifier that is going to be used here is going to be a Linear Support Vector Classification (SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Training model 2\n",
      "Training model 3\n",
      "Training model 4\n",
      "Training model 5\n",
      "Training model 6\n",
      "Training model 7\n",
      "Training model 8\n",
      "Training model 9\n",
      "Training model 10\n",
      "Training model 11\n",
      "Training model 12\n",
      "Training model 13\n",
      "Training model 14\n",
      "Training model 15\n",
      "Training model 16\n",
      "Training model 17\n",
      "Training model 18\n",
      "Training model 19\n",
      "Training model 20\n",
      "[0.5333333333333333, 0.6363636363636364, 0.6060606060606061, 0.6785714285714286, 0.6071428571428571, 0.5, 0.6046511627906976, 0.4, 0.6428571428571429, 0.74, 0.5769230769230769, 0.7407407407407407, 0.56, 0.5909090909090909, 0.5625, 0.6666666666666666, 0.686046511627907, 0.7941176470588235, 0.6716417910447762, 0.7073170731707317]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "l_svc = LinearSVC(max_iter=10000)\n",
    "\n",
    "accuracy = []\n",
    "counter = 0\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    counter += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"Training model\", counter)\n",
    "    l_svc.fit(X_train, y_train)\n",
    "    acc = l_svc.score(X_test, y_test)\n",
    "    accuracy.append(acc)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
